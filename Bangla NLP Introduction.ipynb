{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP In General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teaching computers how to understand (and generate) human language.\n",
    "\n",
    "Language & Communication\n",
    "-----\n",
    " 1. Speaker\n",
    "  - Intention  - Goals, Knowledge, Shared Beliefe\n",
    "  - Generation - tactical\n",
    "  - Synthesis - text or speech\n",
    " 2. Listener\n",
    "  - Perception\n",
    "  - Iterpretetion - Synactic, semantic, pragmatic\n",
    "  - Incorporation - Internalization, understanding  \n",
    " 3. Both\n",
    "  - Context - Grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic NLP pipeline\n",
    "-----\n",
    "\n",
    "**Language** ----U-----> **Computer** -----G----> L**anguage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding Text & Context\n",
    "----\n",
    "\n",
    "**Question:**\n",
    "\n",
    "ঢাকা থেকে **বিছানাকান্দি** ভ্রমনের বিস্তারিত তথ্য যেমন: যাতায়াত ব্যবস্থা, থাকার ব্যবস্থা (ভালমানের হোটেল/মোটেল), দর্শনীয় স্থান, খরচ পাতি ইত্যাদি জানতে ইচ্ছুক? \n",
    "\n",
    "** Answer 01:**\n",
    "\n",
    "এর জন্য প্রথমে আপনাকে ঢাকা থেকে সিলেট যেতে হবে,\n",
    "ঢাকা থেকে সিলেটঃ\n",
    "ট্রেনঃ ঢাকা থেকে সিলেট যাওয়ার জন্য রাতের ট্রেন কমলাপুর থেকে ছাড়ে রাত ১০টায়। সিলেট পৌঁছায় সকাল ৭টায়। ভাড়া ২৯৫ টাকা।\n",
    "বাসঃ শ্যামলী,হানিফ, গ্রীন লাইন, সোহাগ, সাউদিয়া, এস আলম, এনা (ঘোড়াশাল-টঙ্গী রুট)। ভাড়া ৪৫০ টাকা।\n",
    "থাকার হোটেল :\n",
    "হোটেলঃ সিলেট শহরে থাকার জন্য অনেকভালো মানের হোটেল আছে। শহরের নাইওরপুল এলাকায় হোটেল ফরচুন গার্ডেন (০৮২১-৭১৫৫৯০)।\n",
    "জেল সড়কে হোটেল ডালাস (০৮২১-৭২০৯৪৫)। ভিআইপি সড়কে হোটেল হিলটাউন (০৮২১-৭১৮২৬৩)। লিঙ্ক রোডে হোটেল গার্ডেন ইন (০৮২১-৮১৪৫০৭)।\n",
    "আম্বরখানায় হোটেল পলাশ (০৮২১-৭১৮৩০৯)। দরগা এলাকায় হোটেল দরগাগেইট (০৮২১-৭১৭০৬৬)। হোটেল উর্মি (০৮২১-৭১৪৫৬৩)।\n",
    "জিন্দাবাজারে হোটেল মুন লাইট (০৮২১-৭১৪৮৫০)। তালতলায় গুলশান সেন্টার (০৮২১-৭১০০১৮) ইত্যাদি।\n",
    "ভাড়া ৩০০ থেকে শুরু করে ৩০০০টাকা পর্যন্ত,নিরাপত্তাও ভাল আছে হোটেলগুলোতে,দরগাগেটে আরো কয়েকটি ভাল হোটেল আছে।\n",
    "আবার নলজুড়ি উপজেলা সরকারি ডাকবাংলোঃ পূর্ব অনুমতি সাপেক্ষে এইখানে থাকতে পারেন। সরকারি কর্মকর্তা/কর্মচারিদের জন্য প্রতিটি রুম ৫০০টাকা। আর সিভিলিয়ানদের জন্য ১৫০০টাকা।\n",
    "খাবার :\n",
    "খাবার রেস্তোরাঃ পানশি, পাঁচ ভাই জনপ্রিয় দুটি রেস্তোরা। এছাড়া আছে উন্ডাল সহ সব নামি-দামি খাবার দোকান।\n",
    "বিছানা কান্দি :\n",
    "**বিছানাকান্দি** যেতে হলে সর্বপ্রথম আপনাকে নগরীর আম্বরখানা পয়েন্ট যেতে হবে। সেখানে বিমানবন্দর রোডের দিকে সিএনজি স্টেশন আছে। সিএনজি হাদারঘাট নামক জায়গা পর্যন্ত রিজার্ভ করে গেলে ভাল হয়। পাঁচজন মিলে ৪০০টাকায় সাধারণত ভারা নেওয়া হয়। তবে মানুষ কম থাকলে ৮০টাকা জনপ্রতিও যাওয়া যায়। বিমানবন্দর পর্যন্ত রাস্তা অনেক সুন্দর। চারপাশে শুধু সবুজ চা বাগান। নীল আকাশ আর যেন সবুজ কার্পেটের উপর তাবু টানিয়েছে। বিমানবন্দর থেকে সালুটিকর রাস্তায় প্রবেশ করলেই রাস্তা চরম খারাপ। পিচ ঢালা কালো রাজপথে একটু পরপরই ভাঙ্গা গর্ত। মনে হয় একটু আগেই যেন এ পথে গডজিলা হেটে গিয়েছে। ভারী বালুর ট্রাকগুলো রাস্তার এ দশা করেছে। খারাপ রাস্তা পার হয়েই আপনি একদম গ্রামের ভেতর ঢুকে যাবেন। সিলেটের গ্রামগুলো যেমন সবুজ বৃষ্টিতে ধুয়ে রেখেছে। চিকন রাস্তাগুলো সাপের মতোই আঁকাবাঁকা হয়ে গ্রামের মাঝখানে। গ্রাম দেখতে দেখতে আপনি হাদারঘাট এসে পৌছাবেন। হাদারঘাট বাজারটি খুব একটা বড় না আবার ছোটও না।\n",
    "ভাড়ার পরিমান কম বেশি হতে পারে।\n",
    "\n",
    "**Answer 02:**\n",
    "\n",
    "যেভাবে যাবেন:\n",
    "ঢাকা থেকে সিলেটে আপনি ট্রেনে বা বাসে করে যেতে পারেন। সিলেট শহরের যে কোন প্রান্ত থেকে রিজার্ভ করা সিএনজি নিয়ে জেতে হবে হাদার বাজার, ভাড়া নেবে বড়জোর ৫০০ টাকা। সিলেটের আম্বরখানা থেকেও যাওয়া যায় আলাদাভাবে। সেখানে প্রতি সিএনজিতে চারজন করে নেয়া হয় হাদার বাজার পর্যন্ত। ভাড়া জনপ্রতি ৮০ টাকা। সময় লাগবে দেড় ঘণ্টার মত। হাদার বাজার নেমে নৌকা ঠিক করতে হবে **বিছানাকান্দি** পর্যন্ত। ভাড়া নেবে ৪০০-৫০০ টাকা আপ ডাউন। স্থানীয়রা ইদানিং পর্যটক বেসি দেখে নৌকার কৃত্রিম সঙ্কট সৃষ্টি করে নৌকা ভাড়া বেশি নিচ্ছে। সেক্ষেত্রে ৮০০-১০০০ টাকা লাগতে পারে। দামাদামি করে উঠবেন, কারণ হাদার বাজার থেকে **বিছানাকান্দির** দুরত্ব খুব একটা বেশি না। ইঞ্জিন চালিত নৌকায় যেতে সর্বোচ্চ ২০ মিনিট লাগে।\n",
    "যারা ব্যক্তিগত গাড়িতে যাবেন:\n",
    "সিলেট শহর হতে ৬০ কিলোমিটার উত্তর পূর্বে গোয়াইনঘাট উপজেলার রুস্তমপুর ইউনিয়নে বিছানাকান্দি গ্রাম। সিলেট থেকে ভোলাগঞ্জ মহাসড়ক ধরে সালুটিকর বাজারের ডান দিকে গাড়ী নিয়ে গোয়াইনঘাট লিঙ্ক রোডে হয়ে দেড় ঘন্টা গেলেই আপনি পৌঁছে যাবেন বিছানাকান্দি।\n",
    "যেখানে থাকবেন:\n",
    "সিলেট শহরে এখন অনেক গুলো উন্নতমানের হোটেল রয়েছে । বিছানাকান্দিতে দিনে গিয়ে দিনে ফেরা যায় সিলেট শহর থেকে। তাই সিলেট শহরেই থাকতে পারেন। হোটেল হিল টাউন, ব্রিটানিয়া হোটেল, হোটেল দরগা ইন ছাড়াও অনেকগুলো ভাল হোটেল আছে ভিআইপি রোড এবং আম্বরখানা এলাকায়।\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic - বিছানাকান্দি\n",
    "\n",
    "Context - ভ্রমন (ঢাকা থেকে বিছানাকান্দি)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geners Of Text\n",
    "----\n",
    "\n",
    " - Blog\n",
    " - Recepie \n",
    " - Chat \n",
    " - Email\n",
    " - Medical Record\n",
    " - News "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Applicaions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01. Understanding the sentiment (Sentiment Analysis)\n",
    "02. Document Clustering (Finding similar document of interest)\n",
    "03. Intelligent document search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beshto Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Goal\n",
    " - Finding Similar document\n",
    " - Suggest posts of user interest\n",
    " - Meaningful content search result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Similar post -  Bag-Of-Words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_posts = [\"This is a toy post about machine learning. Actually, it contains not much interesting stuff.\",\n",
    "        \"Imaging databases can get huge.\",\n",
    "        \"Most imaging databases safe images permanently.\",\n",
    "        \"Imaging databases store images.\",\n",
    "        \"Imaging databases store images. Imaging databases store images. Imaging databases store images.\"]\n",
    "bn_posts = [\"মহান মুক্তিযুদ্ধে চূড়ান্ত বিজয়ের মাত্র দুদিন আগে ১৪ ডিসেম্বর এই নারকীয় হত্যাযজ্ঞ ঘটিয়েছিল ঘাতকেরা\",\n",
    "           \"আজ ১৪ ডিসেম্বর, শহীদ বুদ্ধিজীবী দিবস\",\n",
    "           \"শহীদ বুদ্ধিজীবী স্মৃতিসৌধে সকালে শ্রদ্ধা জানিয়েছেন রাষ্ট্রপতি মো. আবদুল হামিদ ও প্রধানমন্ত্রী শেখ হাসিনা\",\n",
    "           \"রপুর ও রায়েরবাজারে বুদ্ধিজীবী স্মৃতিসৌধে অগণিত মানুষ পুষ্পস্তবক অর্পণের মাধ্যমে শ্রদ্ধা নিবেদন করছেন\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting raw text into a bag-of-words (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vectorizer = CountVectorizer(min_df=1)\n",
    "en_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 24\n",
      "['about', 'actually', 'can', 'contains', 'databases', 'get', 'huge', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'safe', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "enX_train = en_vectorizer.fit_transform(en_posts)\n",
    "\n",
    "en_num_samples, en_num_features = enX_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (en_num_samples, en_num_features))\n",
    "print(en_vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 4)\t1\n",
      "  (0, 8)\t1\n",
      "[[0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "en_new_post = \"imaging databases\"\n",
    "en_new_post_vec = en_vectorizer.transform([en_new_post])\n",
    "\n",
    "print(type(en_new_post_vec))\n",
    "print(en_new_post_vec)  # coo_matrix\n",
    "print(en_new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **need** to use the **full array** if we want to use it as a vector for similarity calculations. \n",
    "\n",
    "For the similarity measurement (the naive one), we calculate the\n",
    "**Euclidean distance** between the count vectors of the new post and all the old posts as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray()) \n",
    "\n",
    "# The norm() function calculates the Euclidean norm (shortest distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00: \n",
      " This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73: \n",
      " Imaging databases can get huge.\n",
      "=== Post 2 with dist=2.00: \n",
      " Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=1.41: \n",
      " Imaging databases store images.\n",
      "=== Post 4 with dist=5.10: \n",
      " Imaging databases store images. Imaging databases store images. Imaging databases store images.\n"
     ]
    }
   ],
   "source": [
    "##  \"imaging databases\"\n",
    "\n",
    "for i in range(0, en_num_samples):\n",
    "    en_post = en_posts[i]\n",
    "    if en_post == en_new_post:\n",
    "        continue\n",
    "    en_post_vec = enX_train.getrow(i)\n",
    "    d = dist_raw(en_post_vec, en_new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: \\n %s\"%(i, d, en_post))\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best post is  3 with dist=1.41\n"
     ]
    }
   ],
   "source": [
    "print(\"Best post is % i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "[[0 0 0 0 3 0 0 3 3 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(enX_train.getrow(3).toarray())\n",
    "print(enX_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the word count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: \n",
      " This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: \n",
      " Imaging databases can get huge.\n",
      "=== Post 2 with dist=0.92: \n",
      " Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: \n",
      " Imaging databases store images.\n",
      "=== Post 4 with dist=0.77: \n",
      " Imaging databases store images. Imaging databases store images. Imaging databases store images.\n"
     ]
    }
   ],
   "source": [
    "##  \"imaging databases\"\n",
    "\n",
    "for i in range(0, en_num_samples):\n",
    "    en_post = en_posts[i]\n",
    "    if en_post == en_new_post:\n",
    "        continue\n",
    "    en_post_vec = enX_train.getrow(i)\n",
    "    d = dist_norm(en_post_vec, en_new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: \\n %s\"%(i, d, en_post))\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Less important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_vectorizer = CountVectorizer(min_df=1, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00: \n",
      " This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73: \n",
      " Imaging databases can get huge.\n",
      "=== Post 2 with dist=2.00: \n",
      " Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=1.41: \n",
      " Imaging databases store images.\n",
      "=== Post 4 with dist=5.10: \n",
      " Imaging databases store images. Imaging databases store images. Imaging databases store images.\n",
      "Best post is  3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "##  \"imaging databases\"\n",
    "\n",
    "for i in range(0, en_num_samples):\n",
    "    en_post = en_posts[i]\n",
    "    if en_post == en_new_post:\n",
    "        continue\n",
    "    en_post_vec = enX_train.getrow(i)\n",
    "    d = dist_raw(en_post_vec, en_new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: \\n %s\"%(i, d, en_post))\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is % i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it has, however, not changed much as our\n",
    "posts are kept short for demonstration purposes. It will become vital when we look\n",
    "at real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "s= nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphic\n"
     ]
    }
   ],
   "source": [
    "print(s.stem(\"graphics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imag\n"
     ]
    }
   ],
   "source": [
    "print(s.stem(\"imaging\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the vectorizer with NLTK's stemmer\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 14\n",
      "['actual', 'contain', 'databas', 'huge', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'safe', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "en_vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "\n",
    "enX_train = en_vectorizer.fit_transform(en_posts)\n",
    "\n",
    "en_num_samples, en_num_features = enX_train.shape\n",
    "\n",
    "print(\"#samples: %d, #features: %d\" % (en_num_samples, en_num_features))\n",
    "print(en_vectorizer.get_feature_names())\n",
    "\n",
    "en_new_post = \"imaging databases\"\n",
    "en_new_post_vec = en_vectorizer.transform([en_new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: \n",
      " This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.61: \n",
      " Imaging databases can get huge.\n",
      "=== Post 2 with dist=0.63: \n",
      " Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.52: \n",
      " Imaging databases store images.\n",
      "=== Post 4 with dist=0.52: \n",
      " Imaging databases store images. Imaging databases store images. Imaging databases store images.\n",
      "Best post is  3 with dist=0.52\n"
     ]
    }
   ],
   "source": [
    "##  \"imaging databases\"\n",
    "\n",
    "for i in range(0, en_num_samples):\n",
    "    en_post = en_posts[i]\n",
    "    if en_post == en_new_post:\n",
    "        continue\n",
    "    en_post_vec = enX_train.getrow(i)\n",
    "    d = dist_norm(en_post_vec, en_new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: \\n %s\"%(i, d, en_post))\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is % i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a reasonable way to extract a compact vector from a noisy textual post.\n",
    "\n",
    "Let us step back for a while to think about what the feature values\n",
    "actually mean.\n",
    "\n",
    "The feature values simply count occurrences of terms in a post. \n",
    "\n",
    "We silently assumed that higher values for a term also mean that the term is of greater importance to the\n",
    "given post.\n",
    "\n",
    "But in rality higher number of occurence doesn't mean the word is important. Important word normally appear less in the document.\n",
    "\n",
    "So if we ant to give importance to the rally important words we have to use TF-IDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF stands for the counting part, while IDF factors in the discounting.\n",
    "\n",
    "Here is a simple implementation - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def tfidf(term, doc, docset):\n",
    "    tf = float(doc.count(term))/sum(w.count(term) for w in docset)\n",
    "    idf = math.log(float(len(docset))/ \n",
    "                   (len([doc for doc in docset if term in doc])))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [a, abb, abc]\n",
    "print(tfidf(\"a\", a, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27031007207210955\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"b\", abb, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0986122886681098\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"c\", abc, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 14\n",
      "['actual', 'contain', 'databas', 'huge', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'safe', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "en_vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')\n",
    "\n",
    "enX_train = en_vectorizer.fit_transform(en_posts)\n",
    "\n",
    "en_num_samples, en_num_features = enX_train.shape\n",
    "\n",
    "print(\"#samples: %d, #features: %d\" % (en_num_samples, en_num_features))\n",
    "print(en_vectorizer.get_feature_names())\n",
    "\n",
    "en_new_post = \"imaging databases\"\n",
    "en_new_post_vec = en_vectorizer.transform([en_new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: \n",
      " This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.87: \n",
      " Imaging databases can get huge.\n",
      "=== Post 2 with dist=0.86: \n",
      " Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.63: \n",
      " Imaging databases store images.\n",
      "=== Post 4 with dist=0.63: \n",
      " Imaging databases store images. Imaging databases store images. Imaging databases store images.\n",
      "Best post is  3 with dist=0.52\n"
     ]
    }
   ],
   "source": [
    "##  \"imaging databases\"\n",
    "\n",
    "for i in range(0, en_num_samples):\n",
    "    en_post = en_posts[i]\n",
    "    if en_post == en_new_post:\n",
    "        continue\n",
    "    en_post_vec = enX_train.getrow(i)\n",
    "    d = dist_norm(en_post_vec, en_new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: \\n %s\"%(i, d, en_post))\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is % i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35355339  0.          0.          0.          0.        ]\n",
      " [ 0.35355339  0.          0.          0.          0.        ]\n",
      " [ 0.          0.440627    0.29746628  0.37660107  0.37660107]\n",
      " [ 0.          0.78210977  0.          0.          0.        ]\n",
      " [ 0.          0.440627    0.59493255  0.75320213  0.75320213]\n",
      " [ 0.35355339  0.          0.          0.          0.        ]\n",
      " [ 0.35355339  0.          0.          0.          0.        ]\n",
      " [ 0.35355339  0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.52800051  0.          0.        ]\n",
      " [ 0.35355339  0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.52800051  0.          0.        ]\n",
      " [ 0.          0.          0.          0.5393127   0.5393127 ]\n",
      " [ 0.35355339  0.          0.          0.          0.        ]\n",
      " [ 0.35355339  0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(enX_train.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01. Tokenizing the text\n",
    " - CountVectorizer\n",
    "02. Remove Stopwords\n",
    "03. Throwing away rare words\n",
    "04. Calculating TF-IDF values from the counts, considering the whole text crpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Similar post in bangla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_posts = [\"মহান মুক্তিযুদ্ধে চূড়ান্ত বিজয়ের মাত্র দুদিন আগে ১৪ ডিসেম্বর এই নারকীয় হত্যাযজ্ঞ ঘটিয়েছিল ঘাতকেরা\",\n",
    "           \"আজ ১৪ ডিসেম্বর, শহীদ বুদ্ধিজীবী দিবস\",\n",
    "           \"শহীদ বুদ্ধিজীবী স্মৃতিসৌধে সকালে শ্রদ্ধা জানিয়েছেন রাষ্ট্রপতি মো. আবদুল হামিদ ও প্রধানমন্ত্রী শেখ হাসিনা\",\n",
    "           \"রপুর ও রায়েরবাজারে বুদ্ধিজীবী স্মৃতিসৌধে অগণিত মানুষ পুষ্পস্তবক অর্পণের মাধ্যমে শ্রদ্ধা নিবেদন করছেন\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "import re\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ে| ের|েই|দের|ছিলেন|ির|ীয়|েও|টি|ও|তে|কে|গুলি|গুলো|গুলা|ছিলেন|ছিল|লেন|কের|ছেন|েরা)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['আগে', 'মাত্র', 'আই', 'অতএব', 'আগামী', 'অবধি', 'অনুযায়ী', 'আদ্যভাগে', 'এই', 'একই', 'একে', 'একটি', 'এখন', 'এখনও', 'এখানে', 'এখানেই', 'এটি', 'এটা', 'এটাই', 'এতটাই', 'এবং', 'একবার', 'এবার', 'এদের', 'এঁদের', 'এমন', 'এমনকী', 'এল', 'এর', 'এরা']\n"
     ]
    }
   ],
   "source": [
    "with open('bn_stopwords.txt') as f: \n",
    "    bn_stopwords = [line.rstrip('\\n') for line in f]\n",
    "    \n",
    "print(bn_stopwords[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=1, tokenizer=word_tokenize,\n",
    "                                   stop_words=bn_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'অগণিত', 'অর্পণের', 'আজ', 'আবদুল', 'ঘটিয়ে', 'ঘাতক', 'চূড়ান্ত', 'জানিয়ে', 'ডিসেম্বর', 'দিবস', 'দুদিন', 'নারক', 'নিবেদন', 'পুষ্পস্তবক', 'প্রধানমন্ত্রী', 'বিজয়ের', 'বুদ্ধিজীবী', 'মহান', 'মানুষ', 'মুক্তিযুদ্ধ', 'মো', 'রপুর', 'রাষ্ট্রপতি', 'রায়েরবাজার', 'শহীদ', 'শেখ', 'শ্রদ্ধা', 'সকাল', 'স্মৃতিসৌধ', 'হত্যাযজ্ঞ', 'হামিদ', 'হাসিনা', '১৪']\n",
      "#samples: 4, #features: 35\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(bn_posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.43551643,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.28545588,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.33999849],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.33999849],\n",
       "       [ 0.        ,  0.43551643,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.28545588,  0.        ],\n",
       "       [ 0.31245141,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.31245141,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.31245141,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.28545588,  0.        ],\n",
       "       [ 0.24634028,  0.34336615,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.43551643,  0.        ,  0.        ],\n",
       "       [ 0.31245141,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.31245141,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.33999849],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.33999849],\n",
       "       [ 0.        ,  0.        ,  0.28545588,  0.        ],\n",
       "       [ 0.31245141,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.27798449,  0.18220279,  0.21701663],\n",
       "       [ 0.31245141,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.33999849],\n",
       "       [ 0.31245141,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.28545588,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.33999849],\n",
       "       [ 0.        ,  0.        ,  0.28545588,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.33999849],\n",
       "       [ 0.        ,  0.34336615,  0.22505669,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.28545588,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.22505669,  0.26805872],\n",
       "       [ 0.        ,  0.        ,  0.28545588,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.22505669,  0.26805872],\n",
       "       [ 0.31245141,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.28545588,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.28545588,  0.        ],\n",
       "       [ 0.24634028,  0.34336615,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.toarray().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_post = \"শহীদ বুদ্ধিজীবী দিবস\" # \"রায়েরবাজার\" #\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: মহান মুক্তিযুদ্ধে চূড়ান্ত বিজয়ের মাত্র দুদিন আগে ১৪ ডিসেম্বর এই নারকীয় হত্যাযজ্ঞ ঘটিয়েছিল ঘাতকেরা\n",
      "=== Post 1 with dist=0.87: আজ ১৪ ডিসেম্বর, শহীদ বুদ্ধিজীবী দিবস\n",
      "=== Post 2 with dist=1.26: শহীদ বুদ্ধিজীবী স্মৃতিসৌধে সকালে শ্রদ্ধা জানিয়েছেন রাষ্ট্রপতি মো. আবদুল হামিদ ও প্রধানমন্ত্রী শেখ হাসিনা\n",
      "=== Post 3 with dist=1.34: রপুর ও রায়েরবাজারে বুদ্ধিজীবী স্মৃতিসৌধে অগণিত মানুষ পুষ্পস্তবক অর্পণের মাধ্যমে শ্রদ্ধা নিবেদন করছেন\n",
      "======  ======\n",
      " Best post is  1 with dist = 0.87\n"
     ]
    }
   ],
   "source": [
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post = bn_posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "    \n",
    "print(\"======  ======\\n Best post is % i with dist =% .2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
